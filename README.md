# ControlNet Deployment

A FastAPI-based deployment of ControlNet for edge detection and image generation.

## Overview

This project provides a containerized deployment of ControlNet with the following key components:

- FastAPI backend for serving the model
- Docker containerization with CUDA support

## System Requirements

Tested configuration:
- NVIDIA GPU with CUDA support
- Docker with NVIDIA Container Toolkit
- Python 3.8+
- CUDA 11.8
- PyTorch 2.1.0

## Project Structure

```
controlnet-deployment/
├── app/ # FastAPI application
│ ├── main.py # API endpoints
│ └── model.py # ControlNet model wrapper
├── annotator/ # Edge detection modules
├── cldm/ # ControlNet model architecture
├── ldm/ # Latent diffusion modules
├── models/ # Model checkpoints
├── inputs/ # Input images and configs
│ ├── images/
│ └── configs/
└── outputs/ # Generated images
```

## Installation

1. Clone the repository:
```bash
git clone https://github.com/rajanish4/controlnet_deployment.git
cd controlnet-deployment
```

2. Download the model:
- Download the ControlNet model from [Hugging Face](https://huggingface.co/lllyasviel/ControlNet/blob/main/models/control_sd15_canny.pth)
- Place the downloaded `control_sd15_canny.pth` file in the `models/` directory

### Using Docker (Recommended)

1. Build and run using docker-compose:
```bash
docker-compose up --build
```

The GUI API will be available at `http://localhost:8000/docs`
Images can be generated by uploading an image and providing prompt parameters manually or by uploading a config file (stored in inputs/configs/parameters.json, ps: just enter parameters.json as the config file name, or add your own json files inside this folder).

### Local Python Installation

1. Install Miniconda from [https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html)

2. Create and activate conda environment:
```bash
conda env create -f environment.yaml
conda activate control_fastapi
```

3. Run the FastAPI server:
```bash
python run.py
```

The GUI API will be available at `http://localhost:8000/docs`

## API Usage

The API provides two endpoints:

### 1. Direct Generation
POST `/generate`
- Accepts image file and generation parameters
- Returns concatenated control (edge) and generated images

Example using api_test.py:
```python
tester = ControlNetAPITester()
tester.test_generate_endpoint('inputs/images/mri_brain.jpg')
```

### 2. Config-based Generation
POST `/generate_from_config`
- Accepts JSON configuration file
- Returns concatenated control and generated images

Example config (inputs/configs/parameters.json):
```json
{
  "image_path": "images/mri_brain.jpg",
  "params": {
    "prompt": "MRI brain scan",
    "image_resolution": 512,
    "ddim_steps": 10
  }
}
```

Example using api_test.py:
```python
tester.test_generate_from_config('parameters.json')
```

## Testing

Run the test script (make sure the server is already running):
```bash
python api_test.py
```

This will test both endpoints and save generated images.

## Generation Parameters

The API accepts the following parameters:
- `prompt`: Text description for image generation.
- `a_prompt`: Additional positive prompt (default: "good quality").
- `n_prompt`: Negative prompt (default: "animal, drawing, ...").
- `num_samples`: Number of images to generate (default: 1).
- `image_resolution`: Output resolution in pixels (default: 128).
- `ddim_steps`: Number of denoising steps (default: 10).
- `guess_mode`: Boolean flag enabling guess mode (default: false).
- `strength`: Strength parameter for image conditioning (default: 1.0).
- `scale`: Guidance scale for conditional generation (default: 9.0).
- `seed`: Random seed (default: 1).
- `eta`: DDIM sampling eta value (default: 0.0).
- `low_threshold`: Low threshold value for Canny edge detection (default: 50).
- `high_threshold`: High threshold value for Canny edge detection (default: 100).

## Development Notes

- Model weights should be placed in the `models/` directory
- Input images go in `inputs/images/`
- Configuration json must match the parameters in the main.py file and go in `inputs/configs/`
- Generated images are saved to `outputs/`